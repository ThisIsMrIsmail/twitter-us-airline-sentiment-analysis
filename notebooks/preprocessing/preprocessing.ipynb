{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f95aa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import re # type: ignore\n",
    "import string # type: ignore\n",
    "import nltk # type: ignore\n",
    "from nltk.corpus import stopwords # type: ignore\n",
    "from nltk.tokenize import word_tokenize # type: ignore\n",
    "from nltk.stem import PorterStemmer # type: ignore\n",
    "from textblob import Word # type: ignore\n",
    "from autocorrect import Speller # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "d20b0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/Tweets.csv')\n",
    "data = df[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85696309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54824f3e",
   "metadata": {},
   "source": [
    "### __Data Cleaning__\n",
    "1. Missing Values\n",
    "2. Data Types\n",
    "3. Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a065d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the description of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring that the \"text\" and \"airline_sentiment\" columns has unique datatypes\n",
    "num_text_types = data['text'].apply(type).nunique()\n",
    "num_sentiment_types = data['airline_sentiment'].apply(type).nunique()\n",
    "print(f\"n of datatypes in 'text': {num_text_types}\")\n",
    "print(f\"n of datatypes in 'airline_sentiment': {num_sentiment_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettting the row value of the duplicated rows in text column\n",
    "duplicate_count = data['text'].duplicated().sum()\n",
    "print(f\"n of duplicate rows in 'text': {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "92d93c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicated rows in \"text\" column\n",
    "data = data.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c54c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the description of the data after dropping the duplicated rows\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371a0b3",
   "metadata": {},
   "source": [
    "### __Text Preprocessing__\n",
    "1. Lowercasing\n",
    "2. URLs Handling\n",
    "3. User Mentions Handling\n",
    "4. English Abbreviations & Slang Handling\n",
    "5. English Contractions Handling\n",
    "6. Punctuation & Special Characters Handling\n",
    "7. Stopwords Handling\n",
    "8. Emoji/Emoticon Handling via regex\n",
    "9. Spell Checking\n",
    "10. Tokenization\n",
    "11. Lemmatization\n",
    "\n",
    "Exploratory visualization\n",
    "    Top 20 most frequent stems (bar chart)\n",
    "    Word cloud of stem frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fb70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all text to lowercase\n",
    "data['text'] = data['text'].str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing URLs\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing user mentions\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'@\\S+', '', x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing abbreviations and slang to their standard forms\n",
    "abbreviation_dict = {\n",
    "    \"bked\": \"booked\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"plz\": \"please\",\n",
    "    \"sfo\": \"san francisco airport\",\n",
    "    \"lax\": \"los angeles airport\",\n",
    "    \"nyc\": \"new york city\",\n",
    "    \"bos\": \"boston\",\n",
    "    \"las\": \"las vegas\",\n",
    "    \"dal\": \"dallas\",\n",
    "    \"dca\": \"washington, d.c.\",\n",
    "    \"lg\": \"likely good\"\n",
    "}\n",
    "\n",
    "def text_std(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in abbreviation_dict:\n",
    "            word = abbreviation_dict[word]\n",
    "        new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "data['text'] = data['text'].apply(text_std)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafa071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling english contractions\n",
    "english_contractions_dict = {\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\", \"needn't\": \"need not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\", \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\", \"who's\": \"who is\", \"who've\": \"who have\", \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def text_std(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in english_contractions_dict:\n",
    "            word = english_contractions_dict[word]\n",
    "        new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "data['text'] = data['text'].apply(text_std)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuation and special characters\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x.lower() not in english_stopwords))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with emojis (btw: data has no emojis or emoticons)\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "import re\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI: # like ðŸ˜Š\n",
    "        if emot in text:\n",
    "            print(emot)\n",
    "            text = text.replace(\n",
    "                emot,\n",
    "                UNICODE_EMOJI[emot]\n",
    "                    .replace(\":\", \"\")\n",
    "                    .replace(\",\", \"\")\n",
    "                    .replace(\"_\", \" \")\n",
    "                ).lower()\n",
    "            \n",
    "    for emo in EMOTICONS_EMO: # like :â€‘)\n",
    "        if emo in text:\n",
    "            print(emo)\n",
    "            text = text.replace(\n",
    "                emo,\n",
    "                EMOTICONS_EMO[emo]\n",
    "                    .replace(\":\", \"\")\n",
    "                    .replace(\",\", \"\")\n",
    "                    .replace(\"_\", \" \")\n",
    "                ).lower()\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(convert_emojis)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a961b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "all_text = ' '.join(data['text'].astype(str).tolist())\n",
    "unique_words = set(all_text.lower().split())\n",
    "\n",
    "# correcting unique words: creating a dict of unique words and their corrected spellings\n",
    "word_corrections = {word: spell(word) for word in unique_words}\n",
    "\n",
    "def correct_sentence_with_map(sentence, corrections_map):\n",
    "    words = sentence.split()\n",
    "    corrected_words = [corrections_map.get(word.lower(), word) for word in words]\n",
    "\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "data['text'] = data['text'].astype(str).apply(lambda x: correct_sentence_with_map(x, word_corrections))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "652f05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text\n",
    "data['text'] = data['text'].apply(word_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing the text\n",
    "data['text'] = data['text'].apply(Word)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7d7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
