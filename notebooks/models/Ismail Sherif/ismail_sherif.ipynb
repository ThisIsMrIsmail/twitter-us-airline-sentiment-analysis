{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import re\n",
    "from textblob import Word\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "from autocorrect import Speller\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ---------------------------------------------\n",
    "# run the following only once to download the nltk data\n",
    "# ---------------------------------------------\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# preprocess text function\n",
    "# ---------------------------------------------\n",
    "def preprocess_text(text):\n",
    "    abbreviation_dict = {\n",
    "        \"u\": \"you\", \"bked\": \"booked\", \"thx\": \"thanks\", \"plz\": \"please\",\n",
    "        \"sfo\": \"san francisco airport\", \"lax\": \"los angeles airport\",\n",
    "        \"nyc\": \"new york city\", \"bos\": \"boston\", \"las\": \"las vegas\",\n",
    "        \"dal\": \"dallas\", \"dca\": \"washington, d.c.\", \"lg\": \"likely good\"\n",
    "    }\n",
    "    english_contractions_dict = {\n",
    "        \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
    "        \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\", \"needn't\": \"need not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "        \"that'd\": \"that would\", \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "        \"where's\": \"where is\", \"who's\": \"who is\", \"who've\": \"who have\", \"won't\": \"will not\",\n",
    "        \"would've\": \"would have\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "    spell = Speller(lang='en')\n",
    "    english_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = [abbreviation_dict.get(word, word) for word in words]\n",
    "    text = \" \".join(new_words)\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = [english_contractions_dict.get(word, word) for word in words]\n",
    "    text = \" \".join(new_words)\n",
    "\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        if emot in text:\n",
    "            text = text.replace(\n",
    "                emot,\n",
    "                \" \" + UNICODE_EMOJI[emot].replace(\":\", \"\").replace(\",\", \"\").replace(\"_\", \" \") + \" \"\n",
    "            ).lower()\n",
    "    for emo in EMOTICONS_EMO:\n",
    "        if emo in text:\n",
    "            text = text.replace(\n",
    "                emo,\n",
    "                \" \" + EMOTICONS_EMO[emo].replace(\":\", \"\").replace(\",\", \"\").replace(\"_\", \" \") + \" \"\n",
    "            ).lower()\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = \" \".join(x for x in text.split() if x.lower() not in english_stopwords)\n",
    "    text = ' '.join(spell(word) for word in text.split())\n",
    "    text = \" \".join(Word(word).lemmatize() for word in text.split())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# evaluate model function\n",
    "# ---------------------------------------------\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    metrics = {\n",
    "        'Training': {\n",
    "            'Accuracy': train_accuracy * 100,\n",
    "            'Precision': train_precision * 100,\n",
    "            'Recall': train_recall * 100,\n",
    "            'F1-score': train_f1 * 100\n",
    "        },\n",
    "        'Testing': {\n",
    "            'Accuracy': test_accuracy * 100,\n",
    "            'Precision': test_precision * 100,\n",
    "            'Recall': test_recall * 100,\n",
    "            'F1-score': test_f1 * 100\n",
    "        }\n",
    "    }    \n",
    "    \n",
    "    print(\"\\nMetrics Differences (Training - Testing):\")\n",
    "    print(f\"Accuracy Diff: {(train_accuracy - test_accuracy)*100:.2f}%\")\n",
    "    print(f\"Precision Diff: {(train_precision - test_precision)*100:.2f}%\")\n",
    "    print(f\"Recall Diff: {(train_recall - test_recall)*100:.2f}%\")\n",
    "    print(f\"F1-Score Diff: {(train_f1 - test_f1)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {train_precision*100:.2f}%\") \n",
    "    print(f\"Recall: {train_recall*100:.2f}%\")\n",
    "    print(f\"F1-Score: {train_f1*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTesting Metrics:\")\n",
    "    print(f\"Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"Precision: {test_precision*100:.2f}%\")\n",
    "    print(f\"Recall: {test_recall*100:.2f}%\") \n",
    "    print(f\"F1-Score: {test_f1*100:.2f}%\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    metrics_train = list(metrics['Training'].values())\n",
    "    metrics_labels = list(metrics['Training'].keys())\n",
    "    ax1.bar(metrics_labels, metrics_train, color=['blue', 'green', 'red', 'purple'])\n",
    "    ax1.set_title('Training Metrics')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.set_ylabel('Score (%)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    for i, v in enumerate(metrics_train):\n",
    "        ax1.text(i, v + 1, f'{v:.2f}%', ha='center')\n",
    "        \n",
    "    metrics_test = list(metrics['Testing'].values())\n",
    "    ax2.bar(metrics_labels, metrics_test, color=['blue', 'green', 'red', 'purple'])\n",
    "    ax2.set_title('Testing Metrics')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_ylabel('Score (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    for i, v in enumerate(metrics_test):\n",
    "        ax2.text(i, v + 1, f'{v:.2f}%', ha='center')        \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred, labels=['positive', 'negative', 'neutral'])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive', 'negative', 'neutral'], yticklabels=['positive', 'negative', 'neutral'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_score = model.predict_proba(X_test)\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "        y_score = np.exp(y_score) / np.sum(np.exp(y_score), axis=1, keepdims=True)\n",
    "    \n",
    "    classes = ['positive', 'negative', 'neutral']\n",
    "    y_test_bin = pd.get_dummies(y_test).values\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(classes)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(len(classes)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve for {classes[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Multi-Class Sentiment Analysis')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/clean_Tweets.csv\")\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# GloVe PyTorch\n",
    "# -------------------------------------------------\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)\n",
    "# max_words = 10000\n",
    "# max_len = 50\n",
    "# tokenizer = word_tokenize\n",
    "# def build_vocab(texts, max_words):\n",
    "#     word_freq = {}\n",
    "#     for text in texts:\n",
    "#         for word in tokenizer(text):\n",
    "#             word_freq[word] = word_freq.get(word, 0) + 1\n",
    "#     vocab = {word: idx + 1 for idx, (word, _) in enumerate(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:max_words-1])}\n",
    "#     vocab[\"<OOV>\"] = 0\n",
    "#     return vocab\n",
    "# vocab = build_vocab(X, max_words)\n",
    "# X_sequences = [[vocab.get(word, vocab[\"<OOV>\"]) for word in tokenizer(text)][:max_len] for text in X]\n",
    "# X_padded = [seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in X_sequences]\n",
    "# X_padded = np.array(X_padded)\n",
    "# embedding_dim = 100\n",
    "# embeddings_index = {}\n",
    "# with open(\"../../../data/glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "#         embeddings_index[word] = coefs\n",
    "# embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "# for word, idx in vocab.items():\n",
    "#     if idx < max_words:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[idx] = embedding_vector\n",
    "# embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# GloVe TensorFlow\n",
    "# -------------------------------------------------\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# data = pd.read_csv(\"clean_Tweets.csv\")\n",
    "# X = data[\"text\"]\n",
    "# y = data[\"airline_sentiment\"]\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)\n",
    "# max_words = 10000\n",
    "# max_len = 50\n",
    "# tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(X)\n",
    "# X_sequences = tokenizer.texts_to_sequences(X)\n",
    "# X_padded = tokenizer.texts_to_sequences(X)\n",
    "# X_padded = pad_sequences(X_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "# embedding_dim = 100\n",
    "# embeddings_index = {}\n",
    "# with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "#         embeddings_index[word] = coefs\n",
    "# embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if i < max_words:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# TF-IDF Vectorizer\n",
    "# -------------------------------------------------\n",
    "# vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "# X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Word2Vec\n",
    "# -------------------------------------------------\n",
    "# w2v_model = Word2Vec(sentences=[text.split() for text in X], vector_size=100, window=5, min_count=1, workers=4)\n",
    "# def text_to_vec(text, model):\n",
    "#     words = text.split()\n",
    "#     word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "#     if len(word_vecs) == 0:\n",
    "#         return np.zeros(model.vector_size)\n",
    "#     return np.mean(word_vecs, axis=0)\n",
    "# X_vectorized = np.array([text_to_vec(text, w2v_model) for text in X])\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# BERT Embeddings\n",
    "# -------------------------------------------------\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# def text_to_bert_vec(text, tokenizer, model):\n",
    "#     inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "# X_vectorized = np.array([text_to_bert_vec(text, tokenizer, model) for text in X])\n",
    "# pd.DataFrame(X_vectorized).to_csv('../../../vector_data/X_bert_embeddings.csv', index=False)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# N-gram Features\n",
    "# -------------------------------------------------\n",
    "# bigram_vectorizer = CountVectorizer(\n",
    "#     ngram_range=(2,2),\n",
    "#     max_features=20000,\n",
    "#     stop_words='english'\n",
    "# )\n",
    "# trigram_vectorizer = CountVectorizer(\n",
    "#     ngram_range=(3,3),\n",
    "#     max_features=20000,\n",
    "#     stop_words='english'\n",
    "# )\n",
    "# X_bigram = bigram_vectorizer.fit_transform(X)\n",
    "# X_trigram = trigram_vectorizer.fit_transform(X)\n",
    "# X_vectorized = np.hstack((X_bigram.toarray(), X_trigram.toarray()))\n",
    "# pd.DataFrame(X_vectorized).to_csv('../../../vector_data/X_ngram_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# TF-IDF with N-grams Vectorizer\n",
    "# -------------------------------------------------\n",
    "# TfidfVectorizer converts text into numerical features using TF-IDF scores\n",
    "# max_features=20000: Limit vocabulary to top 20,000 most frequent words\n",
    "# token_pattern=r\"(?u)\\b\\w+\\b\": Match any word character (letters, digits, underscore)\n",
    "# ngram_range=(1,3): Create features from single words, pairs, and triplets of consecutive words\n",
    "# vectorizer = TfidfVectorizer(max_features=20000, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1,2))\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     max_features=10000,\n",
    "#     token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "#     ngram_range=(1,3)\n",
    "# )\n",
    "# X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class TF_IDF_with_N_Grams_Vectorizer:\n",
    "    def __init__(self, max_features=20000, token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 3)):\n",
    "        self.max_features = max_features\n",
    "        self.token_pattern = re.compile(token_pattern)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vocabulary_ = {}\n",
    "        self.idf_ = {}\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self.token_pattern.findall(text.lower())\n",
    "\n",
    "    def _generate_ngrams(self, tokens):\n",
    "        ngram_tokens = []\n",
    "        min_n, max_n = self.ngram_range\n",
    "        for n in range(min_n, max_n + 1):\n",
    "            ngram_tokens.extend([' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "        return ngram_tokens\n",
    "\n",
    "    def fit(self, raw_documents):\n",
    "        df = defaultdict(int)\n",
    "        doc_count = len(raw_documents)\n",
    "\n",
    "        for doc in raw_documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            ngrams = self._generate_ngrams(tokens)\n",
    "            unique_terms = set(ngrams)\n",
    "            for term in unique_terms:\n",
    "                df[term] += 1\n",
    "\n",
    "        # Calculate IDF and build vocabulary\n",
    "        sorted_terms = sorted(df.items(), key=lambda x: -x[1])[:self.max_features]\n",
    "        self.vocabulary_ = {term: idx for idx, (term, _) in enumerate(sorted_terms)}\n",
    "        self.idf_ = {\n",
    "            term: np.log((1 + doc_count) / (1 + df[term])) + 1.0\n",
    "            for term in self.vocabulary_\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, raw_documents):\n",
    "        n_docs = len(raw_documents)\n",
    "        n_features = len(self.vocabulary_)\n",
    "        X = np.zeros((n_docs, n_features), dtype=np.float32)\n",
    "\n",
    "        for doc_idx, doc in enumerate(raw_documents):\n",
    "            tokens = self._tokenize(doc)\n",
    "            ngrams = self._generate_ngrams(tokens)\n",
    "            tf = Counter(ngrams)\n",
    "\n",
    "            for term, count in tf.items():\n",
    "                if term in self.vocabulary_:\n",
    "                    tf_val = count / len(ngrams)\n",
    "                    idf_val = self.idf_[term]\n",
    "                    tfidf = tf_val * idf_val\n",
    "                    X[doc_idx, self.vocabulary_[term]] = tfidf\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, raw_documents):\n",
    "        self.fit(raw_documents)\n",
    "        return self.transform(raw_documents)\n",
    "\n",
    "\n",
    "# vectorizer = TF_IDF_with_N_Grams_Vectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "vectorizer = TF_IDF_with_N_Grams_Vectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "print(\"TF-IDF Matrix:\\n\", X_vectorized)\n",
    "\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = sampler.fit_resample(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT embeddings from CSV\n",
    "# X = pd.read_csv('../../../vector_data/X_bert_embeddings.csv')\n",
    "# X = pd.read_csv('../../../vector_data/X_ngram_features.csv')\n",
    "\n",
    "# handling data imbalance\n",
    "# sampler = SMOTE(random_state=42)\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "# sampler = RandomUnderSampler(random_state=42)\n",
    "# X_resampled, y_resampled = sampler.fit_resample(X_vectorized, y)\n",
    "\n",
    "# X_resampled, y_resampled = sampler.fit_resample(X_padded, y)\n",
    "X_resampled, y_resampled = sampler.fit_resample(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_counts = y_train.value_counts()\n",
    "sentiment_counts = pd.Series(y_train).value_counts()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sentiment_counts.plot(kind='bar')\n",
    "plt.xlabel('Airline Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Distribution of Airline Sentiments')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_regression_classifier = LogisticRegression(random_state=42, max_iter=2000)\n",
    "# logistic_regression_classifier.fit(X_train, y_train)\n",
    "# evaluate_model(logistic_regression_classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "evaluate_model(naive_bayes_classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with Grid Search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "print(\"\\nBest Random Forest Parameters:\", rf_grid.best_params_)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "evaluate_model(best_rf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_machine_classifier = SVC(kernel='linear', random_state=42)\n",
    "support_vector_machine_classifier.fit(X_train, y_train)\n",
    "evaluate_model(support_vector_machine_classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example text for prediction\n",
    "# example_text = \"This flight was terrible, I will never fly with them again!\"\n",
    "# example_text = \"I'm so happy with the service, I will definitely fly with them again!\"\n",
    "# example_text = \"I'm am going to have my lunch now\"\n",
    "\n",
    "# # Vectorize the example text using the same vectorizer\n",
    "# example_vectorized = vectorizer.transform([example_text])\n",
    "\n",
    "# # Make prediction\n",
    "# # Logistic Regression predictions\n",
    "# lr_prediction = logistic_regression_classifier.predict(example_vectorized)\n",
    "# lr_probability = logistic_regression_classifier.predict_proba(example_vectorized)\n",
    "\n",
    "# # SVM predictions \n",
    "# svm_prediction = support_vector_machine_classifier.predict(example_vectorized)\n",
    "\n",
    "# # Naive Bayes predictions\n",
    "# nb_prediction = naive_bayes_classifier.predict(example_vectorized)\n",
    "# nb_probability = naive_bayes_classifier.predict_proba(example_vectorized)\n",
    "\n",
    "# print(f\"\\nExample text: {example_text}\")\n",
    "# print(\"\\nLogistic Regression:\")\n",
    "# print(f\"Predicted sentiment: {lr_prediction[0]}\")\n",
    "# print(f\"Prediction probabilities: {lr_probability[0]}\")\n",
    "\n",
    "# print(\"\\nSupport Vector Machine:\")\n",
    "# print(f\"Predicted sentiment: {svm_prediction[0]}\")\n",
    "\n",
    "# print(\"\\nNaive Bayes:\")\n",
    "# print(f\"Predicted sentiment: {nb_prediction[0]}\")\n",
    "# print(f\"Prediction probabilities: {nb_probability[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
